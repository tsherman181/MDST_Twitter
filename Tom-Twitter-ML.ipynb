{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "from nltk import tokenize\n",
    "from nltk.test.classify_fixt import setup_module\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.corpus import subjectivity\n",
    "from nltk.sentiment import SentimentAnalyzer\n",
    "from nltk.sentiment.util import *\n",
    "from nltk.classify import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reads in df\n",
    "df = pd.read_csv(\"twitter_data.csv\",encoding=\"ISO-8859-1\",names=[\"target\", \"ids\", \"date\", \"flag\", \"user\", \"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleans df\n",
    "def clean_df():\n",
    "    df = pd.read_csv(\"twitter_data.csv\",encoding=\"ISO-8859-1\",names=[\"target\", \"ids\", \"date\", \"flag\", \"user\", \"text\"])\n",
    "    df = df.dropna()\n",
    "    del df[\"flag\"]\n",
    "    del df[\"user\"]\n",
    "    del df[\"date\"]\n",
    "    if (len(df[\"target\"].unique()) == 1):\n",
    "        del df[\"target\"]\n",
    "    df = df.replace(4, 1)\n",
    "    df[\"split_text\"] = df[\"text\"].str.split()\n",
    "    df[\"final_text\"] = list(zip(df[\"split_text\"], df[\"target\"]))\n",
    "    from nltk.corpus import stopwords\n",
    "    stops = set(stopwords.words('english'))\n",
    "    new_list = []\n",
    "    for sentence in df[\"final_text\"].tolist():\n",
    "        temp_list = []\n",
    "        for word in sentence[0]:\n",
    "            if word not in stops:\n",
    "                if word[0] != '@':\n",
    "                    if \":\" not in word:\n",
    "                        if \"/\" not in word:\n",
    "                            temp_list.append(word)\n",
    "        new_list.append((temp_list,sentence[1]))\n",
    "    df[\"final_text\"] = new_list\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Runs the model, returns the accuracy\n",
    "def run_model(df, classifier, proportion, min_Freq):\n",
    "    df = df.sample(frac = proportion)\n",
    "    # values of original dataframe\n",
    "    train_df = df.sample(frac = 0.8)\n",
    "    # Creating dataframe with\n",
    "    # rest of the 20% values\n",
    "    test_df = df.drop(train_df.index)\n",
    "    sentim_analyzer = SentimentAnalyzer() # Set up model\n",
    "    all_words_neg = sentim_analyzer.all_words([mark_negation(doc) for doc in train_df[\"final_text\"].tolist()])\n",
    "    unigram_feats = sentim_analyzer.unigram_word_feats(all_words_neg, min_freq=min_Freq)\n",
    "    sentim_analyzer.add_feat_extractor(extract_unigram_feats, unigrams=unigram_feats)\n",
    "    training_set = sentim_analyzer.apply_features(train_df[\"final_text\"].tolist())\n",
    "    test_set = sentim_analyzer.apply_features(test_df[\"final_text\"].tolist())\n",
    "    trainer = classifier.train\n",
    "    classifier = sentim_analyzer.train(trainer, training_set)\n",
    "    for key,value in sorted(sentim_analyzer.evaluate(test_set).items()):\n",
    "        print('{0}: {1}'.format(key, value))\n",
    "    return sentim_analyzer.evaluate(test_set)['Accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_all(classifier, proportion, min_Freq):\n",
    "    df = pd.read_csv(\"twitter_data.csv\",encoding=\"ISO-8859-1\",names=[\"target\", \"ids\", \"date\", \"flag\", \"user\", \"text\"])\n",
    "    df = df.dropna()\n",
    "    del df[\"flag\"]\n",
    "    del df[\"user\"]\n",
    "    del df[\"date\"]\n",
    "    if (len(df[\"target\"].unique()) == 1):\n",
    "        del df[\"target\"]\n",
    "    df = df.replace(4, 1)\n",
    "    df[\"split_text\"] = df[\"text\"].str.split()\n",
    "    df[\"final_text\"] = list(zip(df[\"split_text\"], df[\"target\"]))\n",
    "    from nltk.corpus import stopwords\n",
    "    stops = set(stopwords.words('english'))\n",
    "    new_list = []\n",
    "    for sentence in df[\"final_text\"].tolist():\n",
    "        temp_list = []\n",
    "        for word in sentence[0]:\n",
    "            if word not in stops:\n",
    "                if word[0] != '@':\n",
    "                    if \":\" not in word:\n",
    "                        if \"/\" not in word:\n",
    "                            temp_list.append(word)\n",
    "        new_list.append((temp_list,sentence[1]))\n",
    "    df[\"final_text\"] = new_list\n",
    "    df = df.sample(frac = proportion)\n",
    "    # values of original dataframe\n",
    "    train_df = df.sample(frac = 0.8)\n",
    "    # Creating dataframe with\n",
    "    # rest of the 20% values\n",
    "    test_df = df.drop(train_df.index)\n",
    "    sentim_analyzer = SentimentAnalyzer() # Set up model\n",
    "    all_words_neg = sentim_analyzer.all_words([mark_negation(doc) for doc in train_df[\"final_text\"].tolist()])\n",
    "    unigram_feats = sentim_analyzer.unigram_word_feats(all_words_neg, min_freq=min_Freq)\n",
    "    sentim_analyzer.add_feat_extractor(extract_unigram_feats, unigrams=unigram_feats)\n",
    "    training_set = sentim_analyzer.apply_features(train_df[\"final_text\"].tolist())\n",
    "    test_set = sentim_analyzer.apply_features(test_df[\"final_text\"].tolist())\n",
    "    trainer = classifier.train\n",
    "    classifier = sentim_analyzer.train(trainer, training_set)\n",
    "    for key,value in sorted(sentim_analyzer.evaluate(test_set).items()):\n",
    "        print('{0}: {1}'.format(key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = clean_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training classifier\n",
      "Evaluating NaiveBayesClassifier results...\n",
      "Accuracy: 0.54\n",
      "F-measure [0]: 0.3791649093209616\n",
      "F-measure [1]: 0.6346487962273517\n",
      "Precision [0]: 0.571882951653944\n",
      "Precision [1]: 0.5296188898094449\n",
      "Recall [0]: 0.283596214511041\n",
      "Recall [1]: 0.791640866873065\n",
      "Evaluating NaiveBayesClassifier results...\n",
      "0.54\n"
     ]
    }
   ],
   "source": [
    "x = run_model(df, NaiveBayesClassifier, 0.02, 2000)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training classifier\n",
      "Evaluating NaiveBayesClassifier results...\n",
      "Accuracy: 0.627375\n",
      "F-measure [0]: 0.5956869659568697\n",
      "F-measure [1]: 0.6544569375217341\n",
      "Precision [0]: 0.6512455516014235\n",
      "Precision [1]: 0.6099827139152982\n",
      "Recall [0]: 0.548862784303924\n",
      "Recall [1]: 0.705926481620405\n",
      "Evaluating NaiveBayesClassifier results...\n",
      "Training classifier\n",
      "Evaluating NaiveBayesClassifier results...\n",
      "Accuracy: 0.5983125\n",
      "F-measure [0]: 0.5548244095033594\n",
      "F-measure [1]: 0.6340602402778568\n",
      "Precision [0]: 0.6170081651517486\n",
      "Precision [1]: 0.5855505310758229\n",
      "Recall [0]: 0.5040271834885477\n",
      "Recall [1]: 0.6913334988825428\n",
      "Evaluating NaiveBayesClassifier results...\n",
      "Training classifier\n",
      "Evaluating NaiveBayesClassifier results...\n",
      "Accuracy: 0.597625\n",
      "F-measure [0]: 0.5523571130579892\n",
      "F-measure [1]: 0.6345782722215916\n",
      "Precision [0]: 0.6288790373654212\n",
      "Precision [1]: 0.577240809582817\n",
      "Recall [0]: 0.49243739151996035\n",
      "Recall [1]: 0.7045626417948072\n",
      "Evaluating NaiveBayesClassifier results...\n",
      "Training classifier\n",
      "Evaluating NaiveBayesClassifier results...\n",
      "Accuracy: 0.5694375\n",
      "F-measure [0]: 0.505278276481149\n",
      "F-measure [1]: 0.618865836791148\n",
      "Precision [0]: 0.59799422063573\n",
      "Precision [1]: 0.5528318671542948\n",
      "Recall [0]: 0.43745336980850535\n",
      "Recall [1]: 0.7028147775823071\n",
      "Evaluating NaiveBayesClassifier results...\n",
      "Training classifier\n",
      "Evaluating NaiveBayesClassifier results...\n",
      "Accuracy: 0.5504375\n",
      "F-measure [0]: 0.454455821008722\n",
      "F-measure [1]: 0.6176986446983789\n",
      "Precision [0]: 0.5740563326307722\n",
      "Precision [1]: 0.5390038029867359\n",
      "Recall [0]: 0.37609841827768015\n",
      "Recall [1]: 0.7233009708737864\n",
      "Evaluating NaiveBayesClassifier results...\n",
      "Training classifier\n"
     ]
    }
   ],
   "source": [
    "percent_data = [0.05, .1, .15, .2, .25, .3]\n",
    "min_Freqs = [500, 750, 1000, 1500, 2000]\n",
    "results = []\n",
    "\n",
    "for p in percent_data:\n",
    "    for m in min_Freqs:\n",
    "        results.append(((p, m), run_model(df, NaiveBayesClassifier, p, m)))\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
