{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3953821a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Analysis\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#Data Preprocessing and Feature Engineering\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "\n",
    "#Model Selection and Validation\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import confusion_matrix, classification_report,accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f461cc5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 4]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>ids</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target         ids                                               text\n",
       "0       0  1467810369  @switchfoot http://twitpic.com/2y1zl - Awww, t...\n",
       "1       0  1467810672  is upset that he can't update his Facebook by ...\n",
       "2       0  1467810917  @Kenichan I dived many times for the ball. Man...\n",
       "3       0  1467811184    my whole body feels itchy and like its on fire \n",
       "4       0  1467811193  @nationwideclass no, it's not behaving at all...."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"training.1600000.processed.noemoticon.csv\",encoding=\"ISO-8859-1\",names=[\"target\", \"ids\", \"date\", \"flag\", \"user\", \"text\"])\n",
    "#TEMPORARY EDIT TO CHANGE DATA SAMPLE SIZE\n",
    "################################################################\n",
    "\n",
    "df = df.dropna()\n",
    "del df[\"flag\"]\n",
    "del df[\"user\"]\n",
    "del df[\"date\"]\n",
    "if (len(df[\"target\"].unique()) == 1):\n",
    "    del df[\"target\"]\n",
    "else:\n",
    "    print(df[\"target\"].unique())\n",
    "df.head()\n",
    "\n",
    "df = df.replace(4, 1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#We have train and test, now we need to...\n",
    "# Today's goal: Get a model, train it, test it\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b81071",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5809894a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def form_sentence(tweet):\n",
    "    tweet_blob = TextBlob(tweet)\n",
    "    return ' '.join(tweet_blob.words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "10507db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "def no_user_alpha(tweet):\n",
    "    tweet_list = [ele for ele in tweet.split() if ele != 'user']\n",
    "    clean_tokens = [t for t in tweet_list if re.match(r'[^\\W\\d]*$', t)]\n",
    "    clean_s = ' '.join(clean_tokens)\n",
    "    clean_mess = [word for word in clean_s.split() if word.lower() not in stopwords.words('english')]\n",
    "    return clean_mess\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9a72cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "def no_user_alpha(tweet):\n",
    "    tweet_list = [ele for ele in tweet.split() if ele != 'user']\n",
    "    clean_tokens = [t for t in tweet_list if re.match(r'[^\\W\\d]*$', t)]\n",
    "    clean_s = ' '.join(clean_tokens)\n",
    "    clean_mess = [word for word in clean_s.split() if word.lower() not in stopwords.words('english') and word[0] != '@']\n",
    "    return clean_mess\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1195b766",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'be', 'play', 'with', 'my', 'friends', 'with', 'whom', 'I', 'use', 'to', 'play,', 'when', 'you', 'call', 'me', 'yesterday']\n"
     ]
    }
   ],
   "source": [
    "def normalization(tweet_list):\n",
    "        lem = WordNetLemmatizer()\n",
    "        normalized_tweet = []\n",
    "        for word in tweet_list:\n",
    "            normalized_text = lem.lemmatize(word,'v')\n",
    "            normalized_tweet.append(normalized_text)\n",
    "        return normalized_tweet\n",
    "    \n",
    "tweet_list = 'I was playing with my friends with whom I used to play, when you called me yesterday'.split()\n",
    "print(normalization(tweet_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ddb5822",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('bow',CountVectorizer(analyzer='word')),  # strings to token integer counts\n",
    "    ('tfidf', TfidfTransformer()),  # integer counts to weighted TF-IDF scores\n",
    "    ('classifier', MultinomialNB()),  # train on TF-IDF vectors w/ Naive Bayes classifier\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a9a0844e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(frac = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4124cfb5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "315ea9d8",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/v6/6pj05rw96g3ggwlq0ck8mfrr0000gn/T/ipykernel_38205/2293180020.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mcleandata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mlistofwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mno_user_alpha\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mcleandata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlistofwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/v6/6pj05rw96g3ggwlq0ck8mfrr0000gn/T/ipykernel_38205/3427438039.py\u001b[0m in \u001b[0;36mno_user_alpha\u001b[0;34m(tweet)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mclean_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtweet_list\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'[^\\W\\d]*$'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mclean_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclean_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mclean_mess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mclean_s\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'english'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'@'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mclean_mess\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/v6/6pj05rw96g3ggwlq0ck8mfrr0000gn/T/ipykernel_38205/3427438039.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mclean_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtweet_list\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'[^\\W\\d]*$'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mclean_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclean_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mclean_mess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mclean_s\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'english'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'@'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mclean_mess\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Creates clean csv\n",
    "cleandata = []\n",
    "for sentence in df[\"text\"].tolist():\n",
    "    listofwords = no_user_alpha(sentence)\n",
    "    cleandata.append(\" \".join(listofwords))\n",
    "\n",
    "df[\"new_data\"] = cleandata\n",
    "df.head()\n",
    "    \n",
    "    \n",
    "df.to_csv('temporary.csv')      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e1cfc79e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800000\n"
     ]
    }
   ],
   "source": [
    "train_tweets = df.sample(frac = 0.8)\n",
    "test_tweets = df.drop(train_tweets.index)\n",
    "\n",
    "\n",
    "print(df['text'].size)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b9ce96cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(frac = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "243443b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.74      0.78     88126\n",
      "           1       0.72      0.80      0.76     71874\n",
      "\n",
      "    accuracy                           0.77    160000\n",
      "   macro avg       0.77      0.77      0.77    160000\n",
      "weighted avg       0.78      0.77      0.77    160000\n",
      "\n",
      "[[65602 22524]\n",
      " [14229 57645]]\n",
      "0.77029375\n"
     ]
    }
   ],
   "source": [
    "msg_train, msg_test, label_train, label_test = train_test_split(df['text'], df['target'], test_size=0.2)\n",
    "pipeline.fit(msg_train,label_train)\n",
    "predictions = pipeline.predict(msg_test)\n",
    "print(classification_report(predictions,label_test))\n",
    "print(confusion_matrix(predictions,label_test))\n",
    "print(accuracy_score(predictions,label_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6c63ea80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Size 320000, Test Size 80000\n"
     ]
    }
   ],
   "source": [
    "print(\"Training Size {}, Test Size {}\".format(label_train.size,label_test.size))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b7ab11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.49953125 0.50046875]]\n",
      "\n",
      "[[0.49953125 0.50046875]]\n",
      "\n",
      "[[0.49953125 0.50046875]]\n",
      "\n",
      "[[0.49953125 0.50046875]]\n",
      "\n",
      "[[0.95441792 0.04558208]]\n",
      "sad\n",
      "[[0.95441792 0.04558208]]\n",
      "sad\n",
      "[[0.95441792 0.04558208]]\n",
      "sad\n",
      "[[0.95441792 0.04558208]]\n",
      "sad\n",
      "[[0.95441792 0.04558208]]\n",
      "sad\n",
      "[[0.95441792 0.04558208]]\n",
      "sad\n",
      "[[0.95441792 0.04558208]]\n",
      "sad\n",
      "[[0.95441792 0.04558208]]\n",
      "sad\n",
      "[[0.95441792 0.04558208]]\n",
      "sad\n",
      "[[0.95441792 0.04558208]]\n",
      "sad\n",
      "[[0.95441792 0.04558208]]\n",
      "sad\n",
      "[[0.95441792 0.04558208]]\n",
      "sad\n",
      "[[0.95441792 0.04558208]]\n",
      "sad\n",
      "[[0.95441792 0.04558208]]\n",
      "sad\n",
      "[[0.95441792 0.04558208]]\n",
      "sad\n",
      "[[0.95441792 0.04558208]]\n",
      "sad\n",
      "[[0.95441792 0.04558208]]\n",
      "sad\n",
      "[[0.95441792 0.04558208]]\n",
      "sad\n",
      "[[0.95441792 0.04558208]]\n",
      "sad\n",
      "[[0.95441792 0.04558208]]\n",
      "sad\n",
      "[[0.95441792 0.04558208]]\n",
      "sad\n",
      "[[0.49953125 0.50046875]]\n",
      "F\n",
      "[[0.63204117 0.36795883]]\n",
      "Fu\n",
      "[[0.49953125 0.50046875]]\n",
      "h\n",
      "[[0.215693 0.784307]]\n",
      "happy\n",
      "[[0.215693 0.784307]]\n",
      "happy\n",
      "[[0.215693 0.784307]]\n",
      "happy\n",
      "[[0.215693 0.784307]]\n",
      "happy\n",
      "[[0.215693 0.784307]]\n",
      "happy\n",
      "[[0.49953125 0.50046875]]\n",
      "\n",
      "[[0.49953125 0.50046875]]\n",
      "\n",
      "[[0.49953125 0.50046875]]\n",
      "\n",
      "[[0.49953125 0.50046875]]\n",
      "\n",
      "[[0.49953125 0.50046875]]\n",
      "\n",
      "[[0.49953125 0.50046875]]\n",
      "\n",
      "[[0.49953125 0.50046875]]\n",
      "\n",
      "[[0.49953125 0.50046875]]\n",
      "\n",
      "[[0.49953125 0.50046875]]\n",
      "\n",
      "[[0.49953125 0.50046875]]\n",
      "\n",
      "[[0.71570158 0.28429842]]\n",
      "fuck\n",
      "[[0.71570158 0.28429842]]\n",
      "fuck\n",
      "[[0.49953125 0.50046875]]\n",
      "\n",
      "[[0.215693 0.784307]]\n",
      "happy\n",
      "[[0.215693 0.784307]]\n",
      "happy\n",
      "[[0.49953125 0.50046875]]\n",
      "\n",
      "[[0.49953125 0.50046875]]\n",
      "\n",
      "[[0.49953125 0.50046875]]\n",
      "\n",
      "[[0.49953125 0.50046875]]\n",
      "\n",
      "[[0.49953125 0.50046875]]\n",
      "\n",
      "[[0.49953125 0.50046875]]\n",
      "\n",
      "[[0.49953125 0.50046875]]\n",
      "\n",
      "[[0.49953125 0.50046875]]\n",
      "\n",
      "[[0.49953125 0.50046875]]\n",
      "\n",
      "[[0.49953125 0.50046875]]\n",
      "\n",
      "[[0.49953125 0.50046875]]\n",
      "\n",
      "[[0.49953125 0.50046875]]\n",
      "\n",
      "[[0.49953125 0.50046875]]\n",
      "\n",
      "[[0.49953125 0.50046875]]\n",
      "\n",
      "[[0.49953125 0.50046875]]\n",
      "\n",
      "[[0.49953125 0.50046875]]\n",
      "\n",
      "[[0.49953125 0.50046875]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tkinter as tk  \n",
    "from tkinter import ttk\n",
    "from PIL import ImageTk, Image  \n",
    "import time\n",
    "\n",
    "\n",
    "running = True\n",
    "win = tk.Tk()# Application Name  \n",
    "win.title(\"Python GUI App\")# Label\n",
    "win.geometry(\"500x500\")\n",
    "win.configure(bg=\"black\")\n",
    "  \n",
    "\n",
    "lbl = ttk.Label(win, text = \"Enter the name:\").grid(column = 0, row = 0)# Click event \n",
    "def outputanalysis():\n",
    "    text = [name.get()]\n",
    "    print(pipeline.predict_proba(text))\n",
    "    if running:\n",
    "        print(name.get())\n",
    "    win.after(1000,outputanalysis)\n",
    "    \n",
    "    if(pipeline.predict([name.get()])[0] > 0.5):\n",
    "        output = ttk.Label(win, font=(\"Arial\", 25) ,text = \"Output the number: Happy {}\".format(pipeline.predict([name.get()]))).grid(column = 1, row = 2)# Click event\n",
    "        \n",
    "    if(pipeline.predict([name.get()])[0] < 0.5):\n",
    "        output = ttk.Label(win, font=(\"Arial\", 25) ,text = \"Output the number: Sad {}\".format(pipeline.predict([name.get()]))).grid(column = 1, row = 2)# Click event\n",
    "\n",
    "    \n",
    "\n",
    "    #output = ttk.Label(win, text = \"Output the number: {}\".format(pipeline.predict([name.get()]))).grid(column = 1, row = 2)# Click event\n",
    "    \n",
    "    final = ttk.Label(win, text = pipeline.predict_proba(text)).grid(column = 1, row = 3)# Click event \n",
    "def stop():\n",
    "    global running\n",
    "    running = False\n",
    "def start():\n",
    "    global running\n",
    "    outputanalysis()\n",
    "    running = True\n",
    "def click():   \n",
    "    print(\"Hi,\" + name.get())# Textbox widget  \n",
    "name = tk.StringVar()  \n",
    "nameEntered = ttk.Entry(win, width = 12, textvariable = name).grid(column = 0, row = 1)# Button widget  \n",
    "\n",
    "\n",
    "happyimage = Image.open(\"happy.jpeg\")\n",
    "happyimage = happyimage.resize((90,90), Image.ANTIALIAS)\n",
    "\n",
    "mdstlogo = Image.open(\"mdstlogo.png\")\n",
    "mdstlogo = mdstlogo.resize((350,350),Image.ANTIALIAS)\n",
    "fulllogo = ImageTk.PhotoImage(mdstlogo)\n",
    "mdstlogo = tk.Label(image=fulllogo)\n",
    "\n",
    "\n",
    "\n",
    "test = ImageTk.PhotoImage(happyimage)\n",
    "imagelabel = tk.Label(image=test)\n",
    "imagelabel.image = test\n",
    "button = ttk.Button(win, text = \"submit\", command = click).grid(column = 1, row = 1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "start = ttk.Button(win, text = \"Start Analysis\", command=start)\n",
    "stop = ttk.Button(win, text = \"Stop Analysis\", command=stop)\n",
    "final = ttk.Label(win, text = \"Show Log Statistics\").grid(column = 1, row = 2)\n",
    "mdstlogo.place(x=100,y=90)\n",
    "imagelabel.place(x=0,y=120)\n",
    "\n",
    "\n",
    "\n",
    "#output = ttk.Label(win, text = \"Output the number: {}\".format(pipeline.predict([name.get()]))).grid(column = 1, row = 2)# Click event \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#start.grid()\n",
    "#stop.grid()\n",
    "\n",
    "\n",
    "outputanalysis()\n",
    "\n",
    "win.mainloop()\n",
    "\n",
    "\n",
    "\n",
    "text = [\"Donald Trump amazing\"]\n",
    "print(text)\n",
    "print(pipeline.predict(text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d853ae65",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pos = df[df['target'] == 1]\n",
    "data_neg = df[df['target'] == 0]\n",
    "print(msg_train)\n",
    "data_neg = data_neg\n",
    "neg_text = data_neg['text']\n",
    "plt.figure(figsize = (20,20))\n",
    "wc = WordCloud(max_words = 1000 , width = 1600 , height = 800,\n",
    "               collocations=False).generate(\" \".join(neg_text))\n",
    "plt.imshow(wc)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef04755",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "temp = list(string.ascii_uppercase)\n",
    "print(pipeline.predict([\"dope\"]))\n",
    "for i in temp:\n",
    "    text = [i]\n",
    "    print(text)\n",
    "    print(pipeline.predict(text))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "701b6950",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_processing(tweet):\n",
    "    \n",
    "    #Generating the list of words in the tweet (hastags and other punctuations removed)\n",
    "    def form_sentence(tweet):\n",
    "        tweet_blob = TextBlob(tweet)\n",
    "        return ' '.join(tweet_blob.words)\n",
    "    new_tweet = form_sentence(tweet)\n",
    "    #Removing stopwords and words with unusual symbols\n",
    "    def no_user_alpha(tweet):\n",
    "        tweet_list = [ele for ele in tweet.split() if ele != 'user']\n",
    "        clean_tokens = [t for t in tweet_list if re.match(r'[^\\W\\d]*$', t)]\n",
    "        clean_s = ' '.join(clean_tokens)\n",
    "        clean_mess = [word for word in clean_s.split() if word.lower() not in stopwords.words('english')]\n",
    "        return clean_mess\n",
    "    no_punc_tweet = no_user_alpha(new_tweet)\n",
    "    \n",
    "    #Normalizing the words in tweets \n",
    "    def normalization(tweet_list):\n",
    "        lem = WordNetLemmatizer()\n",
    "        normalized_tweet = []\n",
    "        for word in tweet_list:\n",
    "            normalized_text = lem.lemmatize(word,'v')\n",
    "            normalized_tweet.append(normalized_text)\n",
    "        return normalized_tweet\n",
    "    \n",
    "    \n",
    "    return normalization(no_punc_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c49ddcdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tweets = train_tweets.sample(frac = 0.1)\n",
    "test_tweets = train_tweets.drop(train_tweets.index)\n",
    "\n",
    "train_tweets.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c8243d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_tweets['parsed_text'] = train_tweets['text'].apply(text_processing)\n",
    "test_tweets['parsed_text'] = test_tweets['text'].apply(text_processing)\n",
    "\n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5a2686",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_tweets['parsed_text']\n",
    "y = train_tweets['target']\n",
    "test = test_tweets['parsed_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5090a67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37abf41",
   "metadata": {},
   "outputs": [],
   "source": [
    "newlist = []\n",
    "separator = ' '\n",
    "for sentence in train_tweets[\"parsed_text\"]:\n",
    "    sent = separator.join(sentence)\n",
    "    newlist.append(sent)\n",
    "    \n",
    "\n",
    "print(len(newlist))\n",
    "train_tweets['filtered_sentence'] = newlist\n",
    "print(train_tweets[\"parsed_text\"].size)\n",
    "train_tweets.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2846ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "msg_train, msg_test, label_train, label_test = train_test_split(train_tweets['filtered_sentence'], train_tweets['target'], test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d865f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Machine Learning Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "#Machine Learning Pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('bow',CountVectorizer(analyzer='word')),  # strings to token integer counts\n",
    "    ('tfidf', TfidfTransformer()),  # integer counts to weighted TF-IDF scores\n",
    "    ('classifier', MultinomialNB()),  # train on TF-IDF vectors w/ Naive Bayes classifier\n",
    "])\n",
    "pipeline.fit(msg_train,label_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0dfa253",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pipeline.predict(msg_test)\n",
    "\n",
    "print(classification_report(predictions,label_test))\n",
    "print ('\\n')\n",
    "print(confusion_matrix(predictions,label_test))\n",
    "print(accuracy_score(predictions,label_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f1ca28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01f955f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
